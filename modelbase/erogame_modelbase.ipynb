{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 参考\n",
        "以下のデモを適用\n",
        "\n",
        "https://developers.google.com/machine-learning/recommendation/labs/movie-rec-programming-exercise?hl=ja\n",
        "\n",
        "## メモ\n",
        "* tensorflowの利用では0から始まる数値のインデックスが必要そうなので事前に整形しておく\n"
      ],
      "metadata": {
        "id": "EXy8Q9q9TkRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "metadata": {
        "id": "mnU3E6fZrM6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AgdRThgk4oOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_df = pd.read_csv('drive/My Drive/dev/20230424_recommend_erogame/userbase_matrix.csv', encoding='utf-8')\n",
        "user_df = pd.read_csv('drive/My Drive/dev/20230424_recommend_erogame/userbase_user_map.csv', encoding='utf-8')\n",
        "game_df = pd.read_csv('drive/My Drive/dev/20230424_recommend_erogame/userbase_game_map.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "d-BJ3lCZfMa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_shape = user_df.shape[0] + 1\n",
        "game_shape = game_df.shape[0] + 1"
      ],
      "metadata": {
        "id": "WVG-I4Dtx5LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## テンソル生成\n",
        "def build_rating_sparse_tensor(scores_df):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    scores_df: a pd.DataFrame with `uid`, `game_id` and `score` columns.\n",
        "  Returns:\n",
        "    a tf.SparseTensor representing the scores matrix.\n",
        "  \"\"\"\n",
        "  indices = scores_df[['uid', 'game_id']].values\n",
        "  values = scores_df['score'].values\n",
        "  return tf.SparseTensor(\n",
        "      indices=indices,\n",
        "      values=values,\n",
        "      dense_shape=[user_shape, game_shape])\n",
        "\n",
        "# 損失計算\n",
        "def sparse_mean_square_error(sparse_scores, user_embeddings, game_embeddings):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    sparse_scores: A SparseTensor rating matrix, of dense_shape [N, M]\n",
        "    user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
        "      dimension, such that U_i is the embedding of user i.\n",
        "    game_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
        "      dimension, such that V_j is the embedding of movie j.\n",
        "  Returns:\n",
        "    A scalar Tensor representing the MSE between the true ratings and the\n",
        "      model's predictions.\n",
        "  \"\"\"\n",
        "  predictions = tf.gather_nd(\n",
        "      tf.matmul(user_embeddings, game_embeddings, transpose_b=True),\n",
        "      sparse_scores.indices)\n",
        "  loss = tf.losses.mean_squared_error(sparse_scores.values, predictions)\n",
        "  return loss\n",
        "\n",
        "# CFModel helper class （強調フィルタリングモデル）\n",
        "class CFModel(object):\n",
        "  \"\"\"Simple class that represents a collaborative filtering model\"\"\"\n",
        "  def __init__(self, embedding_vars, loss, metrics=None):\n",
        "    \"\"\"Initializes a CFModel.\n",
        "    Args:\n",
        "      embedding_vars: A dictionary of tf.Variables.\n",
        "      loss: A float Tensor. The loss to optimize.\n",
        "      metrics: optional list of dictionaries of Tensors. The metrics in each\n",
        "        dictionary will be plotted in a separate figure during training.\n",
        "    \"\"\"\n",
        "    self._embedding_vars = embedding_vars\n",
        "    self._loss = loss\n",
        "    self._metrics = metrics\n",
        "    self._embeddings = {k: None for k in embedding_vars}\n",
        "    self._session = None\n",
        "\n",
        "  @property\n",
        "  def embeddings(self):\n",
        "    \"\"\"The embeddings dictionary.\"\"\"\n",
        "    return self._embeddings\n",
        "\n",
        "  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True,\n",
        "            optimizer=tf.train.GradientDescentOptimizer):\n",
        "    \"\"\"Trains the model.\n",
        "    Args:\n",
        "      iterations: number of iterations to run.\n",
        "      learning_rate: optimizer learning rate.\n",
        "      plot_results: whether to plot the results at the end of training.\n",
        "      optimizer: the optimizer to use. Default to GradientDescentOptimizer.\n",
        "    Returns:\n",
        "      The metrics dictionary evaluated at the last iteration.\n",
        "    \"\"\"\n",
        "    with self._loss.graph.as_default():\n",
        "      opt = optimizer(learning_rate)\n",
        "      train_op = opt.minimize(self._loss)\n",
        "      local_init_op = tf.group(\n",
        "          tf.variables_initializer(opt.variables()),\n",
        "          tf.local_variables_initializer())\n",
        "      if self._session is None:\n",
        "        self._session = tf.Session()\n",
        "        with self._session.as_default():\n",
        "          self._session.run(tf.global_variables_initializer())\n",
        "          self._session.run(tf.tables_initializer())\n",
        "          tf.train.start_queue_runners()\n",
        "\n",
        "    with self._session.as_default():\n",
        "      local_init_op.run()\n",
        "      iterations = []\n",
        "      metrics = self._metrics or ({},)\n",
        "      metrics_vals = [collections.defaultdict(list) for _ in self._metrics]\n",
        "\n",
        "      # Train and append results.\n",
        "      for i in range(num_iterations + 1):\n",
        "        _, results = self._session.run((train_op, metrics))\n",
        "        if (i % 10 == 0) or i == num_iterations:\n",
        "          print(\"\\r iteration %d: \" % i + \", \".join(\n",
        "                [\"%s=%f\" % (k, v) for r in results for k, v in r.items()]),\n",
        "                end='')\n",
        "          iterations.append(i)\n",
        "          for metric_val, result in zip(metrics_vals, results):\n",
        "            for k, v in result.items():\n",
        "              metric_val[k].append(v)\n",
        "\n",
        "      for k, v in self._embedding_vars.items():\n",
        "        self._embeddings[k] = v.eval()\n",
        "\n",
        "      if plot_results:\n",
        "        # Plot the metrics.\n",
        "        num_subplots = len(metrics)+1\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(num_subplots*10, 8)\n",
        "        for i, metric_vals in enumerate(metrics_vals):\n",
        "          ax = fig.add_subplot(1, num_subplots, i+1)\n",
        "          for k, v in metric_vals.items():\n",
        "            ax.plot(iterations, v, label=k)\n",
        "          ax.set_xlim([1, num_iterations])\n",
        "          ax.legend()\n",
        "      return results\n",
        "\n",
        "# モデルのビルド\n",
        "def build_model(scores, embedding_dim=3, init_stddev=1.):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ratings: a DataFrame of the ratings\n",
        "    embedding_dim: the dimension of the embedding vectors.\n",
        "    init_stddev: float, the standard deviation of the random initial embeddings.\n",
        "  Returns:\n",
        "    model: a CFModel.\n",
        "  \"\"\"\n",
        "  # Split the ratings DataFrame into train and test.\n",
        "  train_ratings, test_ratings = split_dataframe(scores)\n",
        "  # SparseTensor representation of the train and test datasets.\n",
        "  A_train = build_rating_sparse_tensor(train_ratings)\n",
        "  A_test = build_rating_sparse_tensor(test_ratings)\n",
        "  # Initialize the embeddings using a normal distribution.\n",
        "  U = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[0], embedding_dim], stddev=init_stddev))\n",
        "  V = tf.Variable(tf.random_normal(\n",
        "      [A_train.dense_shape[1], embedding_dim], stddev=init_stddev))\n",
        "  train_loss = sparse_mean_square_error(A_train, U, V)\n",
        "  test_loss = sparse_mean_square_error(A_test, U, V)\n",
        "  metrics = {\n",
        "      'train_error': train_loss,\n",
        "      'test_error': test_loss\n",
        "  }\n",
        "  embeddings = {\n",
        "      \"user_id\": U,\n",
        "      \"movie_id\": V\n",
        "  }\n",
        "  return CFModel(embeddings, train_loss, [metrics])\n",
        "\n",
        "# Utility to split the data into training and test sets.\n",
        "def split_dataframe(df, holdout_fraction=0.1):\n",
        "  \"\"\"Splits a DataFrame into training and test sets.\n",
        "  Args:\n",
        "    df: a dataframe.\n",
        "    holdout_fraction: fraction of dataframe rows to use in the test set.\n",
        "  Returns:\n",
        "    train: dataframe for training\n",
        "    test: dataframe for testing\n",
        "  \"\"\"\n",
        "  test = df.sample(frac=holdout_fraction, replace=False)\n",
        "  train = df[~df.index.isin(test.index)]\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "5hH7bl7LpcXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(scores_df, embedding_dim=30, init_stddev=0.5)\n",
        "model.train(num_iterations=1000, learning_rate=10.)"
      ],
      "metadata": {
        "id": "UtVDYYTsp0FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USER_RATINGS = True\n",
        "DOT = 'dot'\n",
        "COSINE = 'cosine'\n",
        "def compute_scores(query_embedding, item_embeddings, measure=DOT):\n",
        "  \"\"\"Computes the scores of the candidates given a query.\n",
        "  Args:\n",
        "    query_embedding: a vector of shape [k], representing the query embedding.\n",
        "    item_embeddings: a matrix of shape [N, k], such that row i is the embedding\n",
        "      of item i.\n",
        "    measure: a string specifying the similarity measure to be used. Can be\n",
        "      either DOT or COSINE.\n",
        "  Returns:\n",
        "    scores: a vector of shape [N], such that scores[i] is the score of item i.\n",
        "  \"\"\"\n",
        "  u = query_embedding\n",
        "  V = item_embeddings\n",
        "  if measure == COSINE:\n",
        "    V = V / np.linalg.norm(V, axis=1, keepdims=True)\n",
        "    u = u / np.linalg.norm(u)\n",
        "  scores = u.dot(V.T)\n",
        "  return scores\n",
        "\n",
        "def user_recommendations(model, measure=DOT, exclude_rated=False, k=6):\n",
        "  if USER_RATINGS:\n",
        "    scores = compute_scores(\n",
        "        model.embeddings[\"user_id\"][943], model.embeddings[\"movie_id\"], measure)\n",
        "    score_key = measure + ' score'\n",
        "    df = pd.DataFrame({\n",
        "        score_key: list(scores),\n",
        "        'movie_id': movies['movie_id'],\n",
        "        'titles': movies['title'],\n",
        "        'genres': movies['all_genres'],\n",
        "    })\n",
        "    if exclude_rated:\n",
        "      # remove movies that are already rated\n",
        "      rated_movies = ratings[ratings.user_id == \"943\"][\"movie_id\"].values\n",
        "      df = df[df.movie_id.apply(lambda movie_id: movie_id not in rated_movies)]\n",
        "    display.display(df.sort_values([score_key], ascending=False).head(k))\n",
        "\n",
        "def movie_neighbors(model, title_substring, measure=DOT, k=6):\n",
        "  # Search for movie ids that match the given substring.\n",
        "  ids =  movies[movies['title'].str.contains(title_substring)].index.values\n",
        "  titles = movies.iloc[ids]['title'].values\n",
        "  if len(titles) == 0:\n",
        "    raise ValueError(\"Found no movies with title %s\" % title_substring)\n",
        "  print(\"Nearest neighbors of : %s.\" % titles[0])\n",
        "  if len(titles) > 1:\n",
        "    print(\"[Found more than one matching movie. Other candidates: {}]\".format(\n",
        "        \", \".join(titles[1:])))\n",
        "  movie_id = ids[0]\n",
        "  scores = compute_scores(\n",
        "      model.embeddings[\"movie_id\"][movie_id], model.embeddings[\"movie_id\"],\n",
        "      measure)\n",
        "  score_key = measure + ' score'\n",
        "  df = pd.DataFrame({\n",
        "      score_key: list(scores),\n",
        "      'titles': movies['title'],\n",
        "      'genres': movies['all_genres']\n",
        "  })\n",
        "  display.display(df.sort_values([score_key], ascending=False).head(k))"
      ],
      "metadata": {
        "id": "RNH4blXMghtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}